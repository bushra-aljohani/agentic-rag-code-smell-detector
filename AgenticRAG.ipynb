{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "ICOSn2FXCiJY",
        "gvbko-68VhF3",
        "_r795O614fxV",
        "ebS5vX_Ek7Ze",
        "0VpqrXm_MLi9"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "API_KEY =\n",
        "API_URL = \"https://api.deepseek.com/v1/chat/completions\"\n",
        "\n",
        "class DeepSeekLLM:\n",
        "    class ResponseWrapper:\n",
        "        def __init__(self, text):\n",
        "            self.text = text  # ‚úÖ mimic old behavior\n",
        "\n",
        "    def __init__(self, model_name=\"deepseek-chat\"):\n",
        "        self.model_name = model_name\n",
        "\n",
        "    def complete(self, prompt: str):\n",
        "        headers = {\n",
        "            \"Authorization\": f\"Bearer {API_KEY}\",\n",
        "            \"Content-Type\": \"application/json\"\n",
        "        }\n",
        "\n",
        "        data = {\n",
        "            \"model\": self.model_name,\n",
        "            \"messages\": [\n",
        "                {\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": (\n",
        "                        \"You are a senior software quality assistant. \"\n",
        "                        \"Analyze the provided metrics and retrieved examples for code smells. \"\n",
        "                        \"At the end, state:\\n\"\n",
        "                        \"Detected Code Smell: Yes or No\\n\"\n",
        "                        \"Smell Type: Long Method / Large Class / No Smell\"\n",
        "                    )\n",
        "                },\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ]\n",
        "        }\n",
        "\n",
        "        response = requests.post(API_URL, headers=headers, data=json.dumps(data))\n",
        "        response.raise_for_status()\n",
        "        text = response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
        "        return self.ResponseWrapper(text)\n"
      ],
      "metadata": {
        "id": "RXJs9JG0D2YY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymongo datasets\n",
        "!pip install llama-index\n",
        "!pip install llama-index-retrievers-bm25"
      ],
      "metadata": {
        "id": "TAew3DMQoGOi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import (\n",
        "    VectorStoreIndex,\n",
        "    SimpleDirectoryReader,\n",
        "    StorageContext,\n",
        "    SimpleKeywordTableIndex,\n",
        ")"
      ],
      "metadata": {
        "id": "j3nR8eGuzsLo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from llama_index.core.schema import Document\n",
        "\n",
        "def load_dataset_for_prediction(path):\n",
        "    df = pd.read_csv(path)\n",
        "    docs = []\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        text = f\"Summary of metrics:\\n{row['metrics_summary']}\\n Explantaion:\\n{row['smell_explanation']}\"\n",
        "\n",
        "        metadata = {\n",
        "            \"smell_type\": row.get(\"smell_type\", None),  # use for eval or filtering\n",
        "            \"language\": row.get(\"language\", \"\"),\n",
        "            \"effort\": row.get(\"effort\", None),\n",
        "            \"volume\": row.get(\"volume\", None)\n",
        "        }\n",
        "\n",
        "        docs.append(Document(text=text, metadata=metadata))\n",
        "\n",
        "    return docs\n",
        "long_method_docs = load_dataset_for_prediction(\"/content/Dataset_Long_Method_RAG_Index.csv\")\n",
        "large_class_docs = load_dataset_for_prediction(\"/content/Dataset_Large Classes_RAG_Index.csv\")\n"
      ],
      "metadata": {
        "id": "WOYHweWoPE1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This is for Long method and we do the same for Large class\n",
        "class LoggingVectorRetriever:\n",
        "    def __init__(self, retriever):\n",
        "        self.retriever = retriever\n",
        "\n",
        "    def retrieve(self, query):\n",
        "        print(f\"ü§ñ [VectorRetriever] Activated for query: {query}\")\n",
        "        results = self.retriever.retrieve(query)\n",
        "\n",
        "        for r in results:\n",
        "            if hasattr(r, \"node\") and hasattr(r.node, \"metadata\"):\n",
        "                r.node.metadata[\"retriever\"] = \"Vector\"\n",
        "            elif hasattr(r, \"metadata\"):\n",
        "                r.metadata[\"retriever\"] = \"Vector\"\n",
        "            else:\n",
        "                print(f\"‚ö†Ô∏è Could not tag node of type: {type(r)}\")\n",
        "\n",
        "        return results\n"
      ],
      "metadata": {
        "id": "PNhr1l5zNHPz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector_index = VectorStoreIndex.from_documents(long_method_docs)\n",
        "raw_vector = vector_index.as_retriever(similarity_top_k=5)\n",
        "vector_retriever = LoggingVectorRetriever(raw_vector)\n"
      ],
      "metadata": {
        "id": "oThRVZYWNNjD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LoggingBM25Retriever:\n",
        "    def __init__(self, retriever):\n",
        "        self.retriever = retriever\n",
        "\n",
        "    def retrieve(self, query):\n",
        "        print(f\"üîç [BM25Retriever] Activated for query: {query}\")\n",
        "        results = self.retriever.retrieve(query)\n",
        "\n",
        "        for r in results:\n",
        "            # If it's NodeWithScore ‚Üí tag its node\n",
        "            if hasattr(r, \"node\") and hasattr(r.node, \"metadata\"):\n",
        "                r.node.metadata[\"retriever\"] = \"BM25\"\n",
        "            # If it's a direct TextNode (or Document)\n",
        "            elif hasattr(r, \"metadata\"):\n",
        "                r.metadata[\"retriever\"] = \"BM25\"\n",
        "            else:\n",
        "                print(f\"‚ö†Ô∏è Cannot tag node of type: {type(r)}\")\n",
        "\n",
        "        return results\n"
      ],
      "metadata": {
        "id": "TSpu0OKUNbtx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.retrievers.bm25 import BM25Retriever\n",
        "import Stemmer\n",
        "\n",
        "long_method_bm25_retriever = LoggingBM25Retriever(\n",
        "    BM25Retriever.from_defaults(\n",
        "        nodes=long_method_nodes,\n",
        "        similarity_top_k=5,\n",
        "        stemmer=Stemmer.Stemmer(\"english\"),\n",
        "        language=\"english\"\n",
        "    )\n",
        ")\n"
      ],
      "metadata": {
        "id": "CP3hhvkrNdIV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CustomHybridRetriever (Reusable)"
      ],
      "metadata": {
        "id": "mbCJXz782_nb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.retrievers import BaseRetriever\n",
        "from llama_index.core.base.query_pipeline.query import QueryBundle\n",
        "from llama_index.core.schema import NodeWithScore\n",
        "from typing import List\n",
        "\n",
        "class CustomHybridRetriever(BaseRetriever):\n",
        "    def __init__(self, bm25, vector):\n",
        "        super().__init__()\n",
        "        self.bm25 = bm25\n",
        "        self.vector = vector\n",
        "\n",
        "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
        "        print(f\"\\nüìò [HybridRetriever] Activated for query: {query_bundle.query_str}\")\n",
        "\n",
        "        # BM25 Retrieval (already logs and tags)\n",
        "        bm25_results = self.bm25.retrieve(query_bundle.query_str)\n",
        "\n",
        "        # Vector Retrieval (already logs and tags)\n",
        "        vector_results = self.vector.retrieve(query_bundle.query_str)\n",
        "\n",
        "        print(f\"‚úÖ [HybridRetriever] BM25 returned {len(bm25_results)} | Vector returned {len(vector_results)}\\n\")\n",
        "\n",
        "        return bm25_results[:3] + vector_results[:3]\n"
      ],
      "metadata": {
        "id": "lHWnEywX273F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.tools import RetrieverTool\n",
        "\n",
        "# Wrap each retriever in a RetrieverTool\n",
        "long_method_bm25_tool = RetrieverTool.from_defaults(\n",
        "    retriever=long_method_bm25_retriever,\n",
        "    description=\"BM25 for long method smells (keyword-based matching)\"\n",
        ")\n",
        "\n",
        "long_method_vector_tool = RetrieverTool.from_defaults(\n",
        "    retriever=vector_retriever,\n",
        "    description=\"Vector retriever for semantic queries about long method smells\"\n",
        ")\n",
        "\n",
        "long_method_hybrid_tool = RetrieverTool.from_defaults(\n",
        "    retriever=CustomHybridRetriever(\n",
        "        bm25=long_method_bm25_retriever,\n",
        "        vector=vector_retriever\n",
        "    ),\n",
        "    description=\"Hybrid (BM25 + Vector) for long method smell explanation\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "U4YcPMA45Aq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.tools import RetrieverTool\n"
      ],
      "metadata": {
        "id": "XjeU9aXY4S1g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.retrievers import RouterRetriever\n",
        "from llama_index.core.selectors import PydanticSingleSelector\n",
        "from llama_index.core.tools import RetrieverTool"
      ],
      "metadata": {
        "id": "buqd418J8EbP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.llms.openai import OpenAI"
      ],
      "metadata": {
        "id": "T07RRKNmPJN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "long_method_bm25_tool.name = \"BM25\"\n",
        "long_method_vector_tool.name = \"Vector\"\n",
        "long_method_hybrid_tool.name = \"Hybrid\"\n"
      ],
      "metadata": {
        "id": "quXUylkwR-91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.selectors import PydanticSingleSelector\n",
        "from llama_index.core.retrievers import RouterRetriever\n",
        "\n",
        "selector = PydanticSingleSelector.from_defaults(llm=llm)\n",
        "\n",
        "router = RouterRetriever(\n",
        "    retriever_tools=[\n",
        "        long_method_bm25_tool,\n",
        "        long_method_vector_tool,\n",
        "        long_method_hybrid_tool\n",
        "    ],\n",
        "    selector=selector\n",
        ")\n"
      ],
      "metadata": {
        "id": "r9kKbDuHPW4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RouterAgent with Logging"
      ],
      "metadata": {
        "id": "1q7QruY-Jea0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RouterAgent:\n",
        "    def __init__(self, long_method_router, large_class_router, llm):\n",
        "        self.long_method_router = long_method_router\n",
        "        self.large_class_router = large_class_router\n",
        "        self.llm = llm\n",
        "\n",
        "    def classify_smell_type(self, code_str, metrics_text, query):\n",
        "        prompt = f\"\"\"\n",
        "You are a code analysis expert. Based on the provided code, metrics, and the user's question, determine the most relevant code smell type.\n",
        "\n",
        "---\n",
        "Code:\n",
        "{code_str}\n",
        "\n",
        "---\n",
        "Metrics:\n",
        "{metrics_text}\n",
        "\n",
        "---\n",
        "Question:\n",
        "{query}\n",
        "\n",
        "Which type does it represent more clearly?\n",
        "- Long Method\n",
        "- Large Class\n",
        "\n",
        "Only answer with one: Long Method or Large Class.\n",
        "\"\"\"\n",
        "        print(\"üß† Invoking LLM to classify smell type...\")\n",
        "        response = self.llm.complete(prompt).text.strip()\n",
        "        print(f\"üìå LLM Response for Classification: {response}\")\n",
        "        return response.lower()\n",
        "\n",
        "    def retrieve_context(self, code_str, metrics_text, query):\n",
        "        smell_type = self.classify_smell_type(code_str, metrics_text, query)\n",
        "\n",
        "        if \"long\" in smell_type:\n",
        "            print(\"üîç Routing to Long Method Retriever...\")\n",
        "            nodes = self.long_method_router.retrieve(query)\n",
        "        elif \"class\" in smell_type:\n",
        "            print(\"üîç Routing to Large Class Retriever...\")\n",
        "            nodes = self.large_class_router.retrieve(query)\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è Smell Type not recognized\")\n",
        "\n",
        "        print(f\"‚úÖ Retrieved {len(nodes)} nodes.\")\n",
        "        for i, node in enumerate(nodes):\n",
        "            source = node.node.metadata.get(\"retriever\", \"Unknown\") if hasattr(node, \"node\") else node.metadata.get(\"retriever\", \"Unknown\")\n",
        "            text_snippet = node.node.text if hasattr(node, \"node\") else node.text\n",
        "            print(f\"   üîó Node {i+1}: Source={source}, Text Snippet={text_snippet[:60]}...\")\n",
        "        return nodes, smell_type\n"
      ],
      "metadata": {
        "id": "wRjdvITVJfpN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "router_agent = RouterAgent(\n",
        "    long_method_router=router,       # Your Long Method router\n",
        "    large_class_router=router_lc,    # Your Large Class router\n",
        "    llm=llm                          # Your OpenAI or other LLM instance\n",
        ")\n"
      ],
      "metadata": {
        "id": "AE_TrcxAJz1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import time\n",
        "\n",
        "\n",
        "csv_file = \"\"\n",
        "df = pd.read_csv(csv_file).reset_index(drop=True)\n",
        "\n",
        "results = []\n",
        "user_query = \"Does the code present any code smell and if so what is it Long method or Large Class?\"\n",
        "\n",
        "start_time = time.time()\n",
        "batch_start_time = start_time  # for tracking time per batch\n",
        "\n",
        "output_file = \"DeepSeek_Full_Ablation_Results.csv\"\n",
        "\n",
        "print(f\"‚úÖ Starting Ablation Study on {len(df)} rows...\\n\")\n",
        "\n",
        "for idx, row in df.iterrows():\n",
        "    row_start = time.time()\n",
        "\n",
        "    console_log = []\n",
        "    log = lambda msg: (print(msg), console_log.append(msg))\n",
        "\n",
        "    log(f\"üñ•Ô∏è Processing Row {idx+1}/{len(df)} | Unique ID: {row['unique_id']}\")\n",
        "    log(\"üß† Invoking LLM to classify smell type...\")\n",
        "\n",
        "    code_str = row['metrics_summary']\n",
        "\n",
        "    # ‚úÖ No retrieval for ablation\n",
        "    retrieved_nodes = []\n",
        "    log(\"\\n‚úÖ Retrieved Nodes: None (Ablation Study)\")\n",
        "    retrieved_docs_log = []\n",
        "\n",
        "    llm = DeepSeekLLM(model_name=\"deepseek-chat\")\n",
        "    final_answer = detect_code_smell_with_llm(\n",
        "        code_str=code_str,\n",
        "        retrieved_nodes=retrieved_nodes,\n",
        "        llm=llm\n",
        "    )\n",
        "\n",
        "    log(\"\\nüß™ Final LLM Answer:\")\n",
        "    log(final_answer)\n",
        "\n",
        "    # ‚úÖ Extract smell classification\n",
        "    detected_smell_match = re.search(r\"Smell Type:\\s*(.*)\", final_answer)\n",
        "    detected_smell = detected_smell_match.group(1).strip() if detected_smell_match else \"Not Parsed\"\n",
        "\n",
        "    classification = detected_smell\n",
        "    routing = (\n",
        "        \"Long Method Retriever\" if \"Long Method\" in classification\n",
        "        else \"Large Class Retriever\" if \"Large Class\" in classification\n",
        "        else \"No Retriever (No Smell)\" if \"No Smell\" in classification\n",
        "        else \"Unknown\"\n",
        "    )\n",
        "\n",
        "    log(f\"üìå LLM Response for Classification: {classification}\")\n",
        "    log(f\"üîç Routing to {routing}...\")\n",
        "\n",
        "    row_time = time.time() - row_start\n",
        "    log(f\"‚è≥ Time taken for this row: {row_time:.2f} seconds\")\n",
        "\n",
        "    # ‚úÖ Append to results\n",
        "    results.append({\n",
        "        \"Unique ID\": row['unique_id'],\n",
        "        \"User Query\": user_query,\n",
        "        \"Console Log\": \"\\n\".join(console_log),\n",
        "        \"LLM Response for Classification\": classification,\n",
        "        \"Routing to Retriever\": routing,\n",
        "        \"Retrieved Docs\": \"\\n\".join(retrieved_docs_log),\n",
        "        \"Full LLM Response\": final_answer,\n",
        "        \"Detected Code Smell by LLM\": detected_smell,\n",
        "        \"Ground Truth\": row['smell_type'],\n",
        "        \"Row Processing Time (s)\": round(row_time, 2)\n",
        "    })\n",
        "\n",
        "    # ‚úÖ Save progress every 100 rows\n",
        "    if (idx + 1) % 100 == 0:\n",
        "        pd.DataFrame(results).to_csv(output_file, index=False)\n",
        "        batch_time = time.time() - batch_start_time\n",
        "        print(f\"\\n‚úÖ Saved {idx+1} rows to {output_file} (Batch Time: {batch_time/60:.2f} minutes)\")\n",
        "        batch_start_time = time.time()\n",
        "\n",
        "# ‚úÖ Final save\n",
        "pd.DataFrame(results).to_csv(output_file, index=False)\n",
        "total_time = time.time() - start_time\n",
        "print(f\"\\n‚úÖ All {len(df)} rows processed in {total_time/60:.2f} minutes.\")\n",
        "print(f\"‚úÖ Final results saved to {output_file}\")\n"
      ],
      "metadata": {
        "id": "qC0lV8M_0a0z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# ‚úÖ Load your saved results\n",
        "df = pd.read_csv(\"/content/DeepSeek_Full_Ablation_Results.csv\")\n",
        "\n",
        "# ‚úÖ Normalize labels (just in case of spacing issues)\n",
        "df[\"Detected Code Smell by LLM\"] = df[\"Detected Code Smell by LLM\"].str.strip()\n",
        "df[\"Ground Truth\"] = df[\"Ground Truth\"].str.strip()\n",
        "\n",
        "# ‚úÖ Filter out rows where detection failed (optional)\n",
        "valid_df = df[df[\"Detected Code Smell by LLM\"] != \"Not Parsed\"]\n",
        "\n",
        "# ‚úÖ Evaluation\n",
        "y_true = valid_df[\"Ground Truth\"]\n",
        "y_pred = valid_df[\"Detected Code Smell by LLM\"]\n",
        "\n",
        "print(\"‚úÖ Accuracy:\", accuracy_score(y_true, y_pred))\n",
        "print(\"\\n‚úÖ Detailed Classification Report:\\n\")\n",
        "print(classification_report(y_true, y_pred, zero_division=0))\n"
      ],
      "metadata": {
        "id": "mzCVdKou1NRX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import time\n",
        "\n",
        "# ‚úÖ Load the full CSV\n",
        "csv_file = \"/content/Merged_Test_Set_LargeClass_LongMethod.csv\"\n",
        "df = pd.read_csv(csv_file)\n",
        "\n",
        "results = []\n",
        "user_query = \"Does the code present any code smell and if so what is it Long method or Large Class?\"\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "for idx, row in df.iterrows():\n",
        "    row_start = time.time()\n",
        "\n",
        "    # ‚úÖ Capture console log for each row\n",
        "    console_log = []\n",
        "    log = lambda msg: (print(msg), console_log.append(msg))\n",
        "\n",
        "    log(f\"\\nüñ•Ô∏è Processing Row {idx+1}/{len(df)} | Unique ID: {row['unique_id']}\")\n",
        "    log(\"üß† Invoking LLM to classify smell type...\")\n",
        "\n",
        "    code_str = row['metrics_summary']\n",
        "\n",
        "    # ‚úÖ Retrieve relevant nodes\n",
        "    retrieved_nodes = router_agent.retrieve_context(\n",
        "        code_str=code_str,\n",
        "        metrics_text=row['metrics_summary'],\n",
        "        query=user_query\n",
        "    )\n",
        "\n",
        "    log(\"\\n‚úÖ Retrieved Nodes:\")\n",
        "    retrieved_docs_log = []\n",
        "    for i, node in enumerate(retrieved_nodes, 1):\n",
        "        snippet = node.node.text if hasattr(node, \"node\") else node.text\n",
        "        log_line = f\"üîó Node {i}: {snippet[:120]}...\"\n",
        "        log(log_line)\n",
        "        retrieved_docs_log.append(log_line)\n",
        "\n",
        "    # ‚úÖ Run detection with DeepSeek\n",
        "    llm = DeepSeekLLM(model_name=\"deepseek-chat\")\n",
        "    final_answer = detect_code_smell_with_llm(\n",
        "        code_str=code_str,\n",
        "        retrieved_nodes=retrieved_nodes,\n",
        "        llm=llm\n",
        "    )\n",
        "\n",
        "    log(\"\\nüß™ Final LLM Answer:\")\n",
        "    log(final_answer)\n",
        "\n",
        "    # ‚úÖ Extract detected smell\n",
        "    detected_smell_match = re.search(r\"Smell Type:\\s*(.*)\", final_answer)\n",
        "    detected_smell = detected_smell_match.group(1).strip() if detected_smell_match else \"Not Parsed\"\n",
        "\n",
        "    classification = detected_smell\n",
        "    routing = (\n",
        "        \"Long Method Retriever\" if \"Long Method\" in classification\n",
        "        else \"Large Class Retriever\" if \"Large Class\" in classification\n",
        "        else \"No Retriever (No Smell)\" if \"No Smell\" in classification\n",
        "        else \"Unknown\"\n",
        "    )\n",
        "\n",
        "    log(f\"üìå LLM Response for Classification: {classification}\")\n",
        "    log(f\"üîç Routing to {routing}...\")\n",
        "\n",
        "    row_time = time.time() - row_start\n",
        "    log(f\"‚è≥ Time taken for this row: {row_time:.2f} seconds\")\n",
        "\n",
        "    # ‚úÖ Append to results\n",
        "    results.append({\n",
        "        \"Unique ID\": row['unique_id'],\n",
        "        \"User Query\": user_query,\n",
        "        \"Console Log\": \"\\n\".join(console_log),\n",
        "        \"LLM Response for Classification\": classification,\n",
        "        \"Routing to Retriever\": routing,\n",
        "        \"Retrieved Docs\": \"\\n\".join(retrieved_docs_log),\n",
        "        \"Full LLM Response\": final_answer,\n",
        "        \"Detected Code Smell by LLM\": detected_smell,\n",
        "        \"Ground Truth\": row['smell_type'],\n",
        "        \"Row Processing Time (s)\": round(row_time, 2)\n",
        "    })\n",
        "\n",
        "# ‚úÖ Save full results to CSV\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df.to_csv(\"Full_Size_DeepSeek_Test_Results.csv\", index=False)\n",
        "\n",
        "total_time = time.time() - start_time\n",
        "print(f\"\\n‚úÖ All {len(df)} rows processed in {total_time/60:.2f} minutes.\")\n",
        "print(\"‚úÖ Results saved to DeepSeek_Full_Test_Results.csv\")\n"
      ],
      "metadata": {
        "id": "s00cFFWNAnZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# ‚úÖ Load your saved results\n",
        "df = pd.read_csv(\"/content/Full_Size_DeepSeek_Test_Results.csv\")\n",
        "\n",
        "# ‚úÖ Normalize labels (just in case of spacing issues)\n",
        "df[\"Detected Code Smell by LLM\"] = df[\"Detected Code Smell by LLM\"].str.strip()\n",
        "df[\"Ground Truth\"] = df[\"Ground Truth\"].str.strip()\n",
        "\n",
        "# ‚úÖ Filter out rows where detection failed (optional)\n",
        "valid_df = df[df[\"Detected Code Smell by LLM\"] != \"Not Parsed\"]\n",
        "\n",
        "# ‚úÖ Evaluation\n",
        "y_true = valid_df[\"Ground Truth\"]\n",
        "y_pred = valid_df[\"Detected Code Smell by LLM\"]\n",
        "\n",
        "print(\"‚úÖ Accuracy:\", accuracy_score(y_true, y_pred))\n",
        "print(\"\\n‚úÖ Detailed Classification Report:\\n\")\n",
        "print(classification_report(y_true, y_pred, zero_division=0))\n"
      ],
      "metadata": {
        "id": "GrtIA9v1Q6M3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install radon\n"
      ],
      "metadata": {
        "id": "0_93uFmtk7Fb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lizard\n"
      ],
      "metadata": {
        "id": "V21fkLCfpbHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import lizard\n",
        "from radon.raw import analyze\n",
        "from radon.metrics import h_visit\n",
        "from textwrap import dedent\n",
        "import tempfile\n",
        "\n",
        "def code_to_metrics(code_str):\n",
        "    code_str = dedent(code_str)\n",
        "\n",
        "    # --- 1. Raw Metrics ---\n",
        "    raw = analyze(code_str)\n",
        "    loc = raw.loc\n",
        "    comments = raw.comments\n",
        "    multi = raw.multi\n",
        "    blank = raw.blank\n",
        "    sloc = raw.sloc\n",
        "    lloc = raw.lloc\n",
        "\n",
        "    # Derived comment ratios\n",
        "    c_per_l = round((comments / loc) * 100, 2) if loc else 0\n",
        "    c_per_s = round((comments / sloc) * 100, 2) if sloc else 0\n",
        "    c_plus_m_per_l = round(((comments + multi) / loc) * 100, 2) if loc else 0\n",
        "\n",
        "    # --- 2. Halstead Metrics (wrap in function) ---\n",
        "    wrapped_code = f\"def _wrapped_fn():\\n\" + \"\\n\".join(\"    \" + line for line in code_str.splitlines())\n",
        "    try:\n",
        "        h_result = h_visit(wrapped_code)\n",
        "        h = h_result[0] if h_result else None\n",
        "    except Exception:\n",
        "        h = None\n",
        "\n",
        "    halstead_metrics = {\n",
        "        \"volume\": getattr(h, \"volume\", 0),\n",
        "        \"effort\": getattr(h, \"effort\", 0),\n",
        "        \"time\": getattr(h, \"time\", 0),\n",
        "        \"bugs\": getattr(h, \"bugs\", 0),\n",
        "        \"Œ∑1\": getattr(h, \"h1\", 0),  # distinct operators\n",
        "        \"Œ∑2\": getattr(h, \"h2\", 0),  # distinct operands\n",
        "        \"N1\": getattr(h, \"N1\", 0),  # total operators\n",
        "        \"N2\": getattr(h, \"N2\", 0)   # total operands\n",
        "    }\n",
        "\n",
        "    # --- 3. Lizard Metrics ---\n",
        "    with tempfile.NamedTemporaryFile(delete=False, suffix=\".py\", mode=\"w\") as f:\n",
        "        f.write(code_str)\n",
        "        temp_path = f.name\n",
        "\n",
        "    lizard_result = lizard.analyze_file(temp_path)\n",
        "    lizard_metrics = []\n",
        "    for func in lizard_result.function_list:\n",
        "        lizard_metrics.append({\n",
        "            \"function_name\": func.name,\n",
        "            \"nloc\": func.nloc,\n",
        "            \"cyclomatic_complexity\": func.cyclomatic_complexity,\n",
        "            \"token_count\": func.token_count,\n",
        "            \"parameter_count\": func.parameter_count,\n",
        "            \"length\": func.length,\n",
        "        })\n",
        "\n",
        "    # Return final result\n",
        "    return {\n",
        "        \"raw\": {\n",
        "            \"loc\": loc,\n",
        "            \"sloc\": sloc,\n",
        "            \"lloc\": lloc,\n",
        "            \"comments\": comments,\n",
        "            \"multi\": multi,\n",
        "            \"blank\": blank,\n",
        "            \"comment_ratio_LOC\": c_per_l,\n",
        "            \"comment_ratio_SLOC\": c_per_s,\n",
        "            \"comment+multi_ratio_LOC\": c_plus_m_per_l,\n",
        "        },\n",
        "        \"halstead\": halstead_metrics,\n",
        "        \"lizard\": lizard_metrics\n",
        "    }\n"
      ],
      "metadata": {
        "id": "GF8gVRuOrNzz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def detect_code_smell_with_llm(code_str, retrieved_nodes, llm, smell_type=\" \"):\n",
        "    user_metrics = extract_metrics_from_summary(code_str)\n",
        "\n",
        "    if smell_type == \"Large Class\":\n",
        "        filtered_examples = filter_retrieved_examples(user_metrics, \"Large Class\", kb_large)\n",
        "    elif smell_type == \"Long Method\":\n",
        "        filtered_examples = filter_retrieved_examples(user_metrics, \"Long Method\", kb_long)\n",
        "    else:\n",
        "        context = \"\\n\\n\".join([\n",
        "            node.node.text if hasattr(node, \"node\") else node.text\n",
        "            for node in retrieved_nodes\n",
        "        ])\n",
        "        filtered_examples = None\n",
        "\n",
        "    if filtered_examples is not None and not filtered_examples.empty:\n",
        "        context = \"\\n\\n\".join(filtered_examples[\"metrics_summary\"].tolist())\n",
        "    else:\n",
        "        context = \"\\n\\n\".join([\n",
        "            node.node.text if hasattr(node, \"node\") else node.text\n",
        "            for node in retrieved_nodes\n",
        "        ])\n",
        "\n",
        "    # ‚úÖ Prompt\n",
        "    prompt = f\"\"\"\n",
        "You are a senior software quality expert.\n",
        "\n",
        "IMPORTANT:\n",
        "1. You MUST analyze the given metrics FIRST.\n",
        "2. Retrieved examples are ONLY for comparison.\n",
        "3. DO NOT reinterpret or modify the given metrics.\n",
        "\n",
        "---\n",
        "### Code Metrics to Analyze:\n",
        "{code_str}\n",
        "\n",
        "---\n",
        "### Retrieved Examples for Comparison:\n",
        "{context}\n",
        "\n",
        "---\n",
        "At the end of your response, write clearly:\n",
        "Detected Code Smell: Yes or No\n",
        "Smell Type: Long Method / Large Class / No Smell\n",
        "\"\"\"\n",
        "\n",
        "    response = llm.complete(prompt)\n",
        "    return response.text\n"
      ],
      "metadata": {
        "id": "ADg0AHaO6s-9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}